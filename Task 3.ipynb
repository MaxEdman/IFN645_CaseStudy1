{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import casestudy_tools as tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from casestudy_tools import preprocess\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocessing step\n",
    "df = tools.preprocess()\n",
    "\n",
    "# random state\n",
    "rs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "y = df['ORGYN']\n",
    "X = df.drop(['ORGYN'], axis=1)\n",
    "X_mat = X.as_matrix()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mat, y, test_size=0.3, stratify=y, random_state=rs)\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# initialise a standard scaler object\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train, y_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=10, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q2. build default regression model - Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(random_state=rs)\n",
    "\n",
    "# fit it to training data\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.8100411416816662\n",
      "Test accuracy: 0.816409179541023\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.95      0.89      5015\n",
      "          1       0.73      0.41      0.53      1652\n",
      "\n",
      "avg / total       0.81      0.82      0.80      6667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training and test accuracy\n",
    "print(\"Train accuracy:\", model.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", model.score(X_test, y_test))\n",
    "\n",
    "# classification report on test data\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.70609843  0.01244589  0.81174692  0.02140138  0.38484826 -0.08521423\n",
      "  -0.39119047 -0.00276403 -0.00340256  0.00816159  0.0056556   0.00411815\n",
      "   0.00521994 -0.01580546 -0.00239069 -0.00089052  0.01434096 -0.02652518\n",
      "   0.02107318 -0.02704544 -0.07418872 -0.07856464 -0.07435022 -0.07893452\n",
      "  -0.06954458 -0.05825127 -0.00952424  0.01377611 -0.01625367 -0.01176906\n",
      "   0.00462889  0.01434096  0.02125866 -0.00418639 -0.01314437 -0.0052902 ]]\n"
     ]
    }
   ],
   "source": [
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['AGE', 'BILL', 'AFFL', 'LTIME', 'GENDER_F', 'GENDER_M', 'GENDER_U',\n",
      "       'TV_REG_Border', 'TV_REG_C Scotland', 'TV_REG_East', 'TV_REG_London',\n",
      "       'TV_REG_Midlands', 'TV_REG_N East', 'TV_REG_N Scot', 'TV_REG_N West',\n",
      "       'TV_REG_S & S East', 'TV_REG_S West', 'TV_REG_Ulster',\n",
      "       'TV_REG_Wales & West', 'TV_REG_Yorkshire', 'NGROUP_A', 'NGROUP_B',\n",
      "       'NGROUP_C', 'NGROUP_D', 'NGROUP_E', 'NGROUP_F', 'NGROUP_U',\n",
      "       'REGION_Midlands', 'REGION_North', 'REGION_Scottish',\n",
      "       'REGION_South East', 'REGION_South West', 'CLASS_Gold',\n",
      "       'CLASS_Platinum', 'CLASS_Silver', 'CLASS_Tin'],\n",
      "      dtype='object')\n",
      "AGE : -0.7060984324791028\n",
      "BILL : 0.01244589088584386\n",
      "AFFL : 0.8117469224600329\n",
      "LTIME : 0.021401384736138115\n",
      "GENDER_F : 0.3848482579826918\n",
      "GENDER_M : -0.08521423027940934\n",
      "GENDER_U : -0.3911904688292492\n",
      "TV_REG_Border : -0.002764027816175187\n",
      "TV_REG_C Scotland : -0.0034025588151601603\n",
      "TV_REG_East : 0.008161586890462787\n",
      "TV_REG_London : 0.005655597057546387\n",
      "TV_REG_Midlands : 0.004118152620581857\n",
      "TV_REG_N East : 0.005219940967963033\n",
      "TV_REG_N Scot : -0.015805456635357828\n",
      "TV_REG_N West : -0.00239069490357791\n",
      "TV_REG_S & S East : -0.0008905223972952225\n",
      "TV_REG_S West : 0.014340962928719938\n",
      "TV_REG_Ulster : -0.02652518331901389\n",
      "TV_REG_Wales & West : 0.021073181523595726\n",
      "TV_REG_Yorkshire : -0.02704543852800193\n"
     ]
    }
   ],
   "source": [
    "feature_names = X.columns\n",
    "coef = model.coef_[0]\n",
    "print(feature_names)\n",
    "\n",
    "# limit to 20 features, you can comment the following line to print out everything\n",
    "coef = coef[:20]\n",
    "\n",
    "for i in range(len(coef)):\n",
    "    print(feature_names[i], ':', coef[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFFL : 0.8117469224600329\n",
      "AGE : -0.7060984324791028\n",
      "GENDER_U : -0.3911904688292492\n",
      "GENDER_F : 0.3848482579826918\n",
      "GENDER_M : -0.08521423027940934\n",
      "NGROUP_D : -0.0789345161933792\n",
      "NGROUP_B : -0.07856464111714075\n",
      "NGROUP_C : -0.07435021627169917\n",
      "NGROUP_A : -0.07418871601564654\n",
      "NGROUP_E : -0.06954457520323662\n",
      "NGROUP_F : -0.058251272357844726\n",
      "TV_REG_Yorkshire : -0.02704543852800193\n",
      "TV_REG_Ulster : -0.02652518331901389\n",
      "LTIME : 0.021401384736138115\n",
      "CLASS_Gold : 0.021258661767927876\n",
      "TV_REG_Wales & West : 0.021073181523595726\n",
      "REGION_North : -0.01625366773642543\n",
      "TV_REG_N Scot : -0.015805456635357828\n",
      "REGION_South West : 0.014340962928719938\n",
      "TV_REG_S West : 0.014340962928719938\n"
     ]
    }
   ],
   "source": [
    "# grab feature importances from the model and feature name from the original X\n",
    "coef = model.coef_[0]\n",
    "feature_names = X.columns\n",
    "\n",
    "# sort them out in descending order\n",
    "indices = np.argsort(np.absolute(coef))\n",
    "indices = np.flip(indices, axis=0)\n",
    "\n",
    "# limit to 20 features, you can leave this out to print out everything\n",
    "indices = indices[:20]\n",
    "\n",
    "for i in indices:\n",
    "    print(feature_names[i], ':', coef[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimal hyperparameters with GridSearchCV model\n",
    "# grid search CV\n",
    "params = {'C': [pow(10, x) for x in range(-6, 4)]}\n",
    "\n",
    "# use all cores to tune logistic regression with C parameter\n",
    "cv = GridSearchCV(param_grid=params, estimator=LogisticRegression(random_state=rs), cv=10, n_jobs=-1)\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "# test the best model\n",
    "print(\"Train accuracy:\", cv.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", cv.score(X_test, y_test))\n",
    "\n",
    "y_pred = cv.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# print parameters of the best model\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most important features\n",
    "import numpy as np\n",
    "\n",
    "coef = cv.best_estimator_.coef_[0]\n",
    "feature_names = X.columns\n",
    "\n",
    "# sort them out in descending order\n",
    "indices = np.argsort(np.absolute(coef))\n",
    "indices = np.flip(indices, axis=0)\n",
    "\n",
    "# limit to 20 features, you can leave this out to print out everything\n",
    "indices = indices[:20]\n",
    "\n",
    "for i in indices:\n",
    "    print(feature_names[i], ':', coef[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3 Feature transformation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# get columns from X\n",
    "cols = X.columns\n",
    "\n",
    "# filter out columns with '_' in their name, as '_' indicate one-hot encoded binary variables\n",
    "cols = [col for col in cols if '_' not in col]\n",
    "\n",
    "\n",
    "\n",
    "# visualise the columns\n",
    "fig, ax = plt.subplots(2,4, figsize=(10,10), sharex=False)\n",
    "\n",
    "# draw distplots on each inspected column in X\n",
    "for i, col in enumerate(cols):\n",
    "    sns.distplot(X[col].dropna(), hist=False, ax=ax[int(i/4)][i%4])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list columns to be transformed\n",
    "columns_to_transform = ['AGE', 'BILL', 'AFFL', 'LTIME']\n",
    "\n",
    "# copy the dataframe\n",
    "X_log = X.copy()\n",
    "df_log = df.copy()\n",
    "\n",
    "# transform the columns with np.log\n",
    "for col in columns_to_transform:\n",
    "    X_log[col] = X_log[col].apply(lambda x: x+1)\n",
    "    X_log[col] = X_log[col].apply(np.log)\n",
    "\n",
    "# plot them again to show the distribution\n",
    "fig, ax = plt.subplots(2,4, figsize=(10,10), sharex=False)\n",
    "for i, col in enumerate(columns_to_transform):\n",
    "    sns.distplot(X_log[col].dropna(), hist=False, ax=ax[int(i/4)][i%4])\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create X, y and train test data partitions\n",
    "# create X, y and train test data partitions\n",
    "y_log = df_log['ORGYN']\n",
    "X_log = df_log.drop(['ORGYN'], axis=1)\n",
    "X_mat_log = X_log.as_matrix()\n",
    "X_train_log, X_test_log, y_train_log, y_test_log = train_test_split(X_mat_log, y_log, test_size=0.3, stratify=y_log, \n",
    "                                                                    random_state=rs)\n",
    "\n",
    "# standardise them again\n",
    "scaler_log = StandardScaler()\n",
    "X_train_log = scaler_log.fit_transform(X_train_log, y_train_log)\n",
    "X_test_log = scaler_log.transform(X_test_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid search CV\n",
    "params = {'C': [pow(10, x) for x in range(-6, 4)]}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=LogisticRegression(random_state=rs), cv=10, n_jobs=-1)\n",
    "cv.fit(X_train_log, y_train_log)\n",
    "\n",
    "# test the best model\n",
    "print(\"Train accuracy:\", cv.score(X_train_log, y_train_log))\n",
    "print(\"Test accuracy:\", cv.score(X_test_log, y_test_log))\n",
    "\n",
    "y_pred = cv.predict(X_test_log)\n",
    "print(classification_report(y_test_log, y_pred))\n",
    "\n",
    "# print parameters of the best model\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3 Feature Transformation\n",
    "from sklearn.feature_selection import RFECV\n",
    "rfe = RFECV(estimator = LogisticRegression(random_state=rs), cv=10)\n",
    "rfe.fit(X_train, y_train) # run the RFECV\n",
    "\n",
    "\n",
    "# comparing how many variables before and after\n",
    "print(\"Original feature set\", X_train.shape[1])\n",
    "print(\"Number of features after elimination\", rfe.n_features_)\n",
    "print(rfe.get_support(indices=True)) \n",
    "#print(feature_names[0,2,4,5,6,28])\n",
    "#print(feature_names(rfe.get_support(indices=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the performance\n",
    "X_train_sel = rfe.transform(X_train)\n",
    "X_test_sel = rfe.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search CV\n",
    "params = {'C': [pow(10, x) for x in range(-6, 4)]}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=LogisticRegression(random_state=rs), cv=10, n_jobs=-1)\n",
    "cv.fit(X_train_sel, y_train)\n",
    "\n",
    "# test the best model\n",
    "print(\"Train accuracy:\", cv.score(X_train_sel, y_train))\n",
    "print(\"Test accuracy:\", cv.score(X_test_sel, y_test))\n",
    "\n",
    "y_pred = cv.predict(X_test_sel)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# print parameters of the best model\n",
    "print(cv.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from casestudy_tools import get_decision_tree\n",
    "#from casestudy_tools import analyse_feature_importance\n",
    "\n",
    "params = {'criterion': ['gini', 'entropy'],\n",
    "          'max_depth': range(2, 5),\n",
    "          'min_samples_leaf': range(1,2)}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=DecisionTreeClassifier(random_state=rs), cv=10)\n",
    "cv.fit(X_train_log, y_train_log)\n",
    "\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets feature importance and relates to the column names of the model\n",
    "feature_importances = cv.best_estimator_.feature_importances_\n",
    "feature_names = X_log.columns\n",
    "\n",
    "# Sorts the features\n",
    "feature_indices = np.flip(np.argsort(feature_importances), axis=0)\n",
    "\n",
    "# Prints the features\n",
    "for i in feature_indices:\n",
    "    print(feature_names[i], ':', feature_importances[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# use the trained best decision tree from GridSearchCV to select features\n",
    "# supply the prefit=True parameter to stop SelectFromModel to re-train the model\n",
    "selectmodel = SelectFromModel(cv.best_estimator_, prefit=True)\n",
    "X_train_sel_model = selectmodel.transform(X_train_log)\n",
    "X_test_sel_model = selectmodel.transform(X_test_log)\n",
    "\n",
    "print(X_train_sel_model.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search cv for RFE SELECTION MODEL (BEST MODEL)\n",
    "params = {'C': [pow(10, x) for x in range(-6, 4)]}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=LogisticRegression(random_state=rs), cv=10, n_jobs=-1)\n",
    "cv.fit(X_train_sel_model, y_train_log)\n",
    "\n",
    "print(\"Train accuracy:\", cv.score(X_train_sel_model, y_train_log))\n",
    "print(\"Test accuracy:\", cv.score(X_test_sel_model, y_test_log))\n",
    "\n",
    "# test the best model\n",
    "y_pred = cv.predict(X_test_sel_model)\n",
    "print(classification_report(y_test_log, y_pred))\n",
    "\n",
    "# print parameters of the best model\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#top 5 variables\n",
    "# Evaluating the feature importance of the default_decision tree\n",
    "import numpy as np\n",
    "\n",
    "coef = cv.best_estimator_.coef_[0]\n",
    "feature_names = X.columns\n",
    "\n",
    "# sort them out in descending order\n",
    "indices = np.argsort(np.absolute(coef))\n",
    "indices = np.flip(indices, axis=0)\n",
    "\n",
    "# limit to 20 features, you can leave this out to print out everything\n",
    "indices = indices[:20]\n",
    "\n",
    "for i in indices:\n",
    "    print(feature_names[i], ':', coef[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
